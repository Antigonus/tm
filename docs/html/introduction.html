<!DOCTYPE HTML>
<html>
<head>
	<title>TM - Introduction</title>
        <link rel="stylesheet" href="style.css" type="text/css" />
        <meta lang="en-US"/>
	<meta name="created" content="2016-03-28T11:07:52.454883632"/>

</head>

<body>
  <header>
    <ul class="nav">
      <li class="nav"><a href="index.html">Top</a></li>
      <li class="nav"><a href="toc.html">Table of Contents</a></li>
    </ul>
    <div class="center">
      <ul class="pagination">
        <li><a href="introduction.html" class="active">1</a></li>
        <li><a href="unicode.html">2</a></li>
        <li><a href="nomenclature.html">3</a></li>
        <li><a href="list-making.html">4</a></li>
        <li><a href="stepping.html">5</a></li>
        <li><a href="working.html">6</a></li>
        <li><a href="spaces.html">7</a></li>
        <li><a href="generators.html">8</a></li>
        <li><a href="transforms.html">9</a></li>
        <li><a href="issues.html">10</a></li>
        <li><a href="dictionary">11</a></li>
      </ul>
    </div>
  </header>

  <article>
    <section>
      <h1>Introduction</h1>

      <p>
        In Pascal's calculator and Babbage's computer the alignment between one of the
        imprinted digit values encircling a gear and an index needle was said to indicate the value of
        a digit.  Then an array of such gears and index pointers were said to indicate the
        values of the multiple digits that made up a number.  Contemporary computers
        create an abstraction in an analogous manner, where a more than threshold amount
        of charge existing in a memory cell is said to indicate a <i>one</i> and a less than
        threshold amount is said to indicate a <i>zero</i>, thus indicating the value of a
        single binary digit, or bit.  Bits are then arrayed to create words, which
        typically represent integers.  

      <p>
        At the next abstraction level up words are then arrayed.  An address decoder is
        placed on top of such an array.  An address decoder is a circuit that creates a
        correspondence between an array of charges, such as those found in words, and the
        location of words.  The address decoder then gives our container topological
        properties such neighboring words, and ordering of words. Note that in this
        context a word is an allocation unit, not a value.  We address memory cells, not
        the data in them.  Thus such a container forms a sort of fabric, which we abstract
        into the concept of address space.

      <p>
        The prior two paragraphs summarize my partially complete "Memory Book" which may
        be found on my website.  The take away here is that the interface between
        computation and information is the <i>container</i>, so containers are of
        fundamental importance. In mathematics the fundamental container is the set, in
        computing it depends on the language used.  In the C language, all pointers can
        be interpreted as arrays of objects that have a given type (a given layout in
        memory, a given format) simply by applying the [·] operator.  Lisp is also built
        upon a fundamental container type, the <i>list</i>, with access through <i>car</i>
        and <i>cdr</i>. 

      <p>
        Lisp programmers came to realize that containers of differing implementations have
        similar properties, and thus could be used through a common interface.  They now
        speak of <i>sequences</i> instead of lists, where a sequence can be either an
        array or a list.  C++ programmers came to a similar realization, and with the
        advent of the Standard Template Library, pointers evolved to general iterators.
        Once incremented, a pointer points to the next element in an array, and once
        incremented, an iterator points to the next member of the STL container.

      <p>
        However, the distillation of computing to that of the processing of data in
        sequence predates C, Lisp, and Fortran for that matter.  The Turing Machine is an
        abstraction of a computing machine built from a state controller, a read-write
        head, and a tape.  The tape is a memory constructed of a array of cells.  The head
        can be moved to a neighbor cell, no differently than moving a pointer or an
        iterator using the '++' and '--' operator.  Though there is a salient difference
        between these paradigms in that the Turing Machine has been formalized.

      <p>
        In <i>Elements of the Theory of Computation</i> Christos Papdimitriou builds
        computation theory up from founding assumptions, while characterizing successively
        more powerful computing paradigms, arriving finally at the Turing Machine. He
        finding limitations, but is not able to identify a more powerful paradigm. In a
        dual approach Lambda Calculus can be used to establish the same theorems.  What we
        know from these formalisms is that all Turing Complete computing approaches employ
        a potentially open series of finite computations, performed over a potentially
        unbounded memory, where that memory need only have one topological property, that
        of a every cell, save one, having two distinct neighbors.  (It can be held that an
        exceptional cell is a start cell, and has only one neighbor.)

      <p>
        Presented in this library is a container type that is a variation on the sequence
        and iteration model combined, while at every boundary triggering, not an
        exception, but a call to a continuation function.  We call our container a <i>tape
        machine</i>. For example, to step an iterator, one calls the function #'s and
        passes it a <i>tape machine</i> and two functions.  Here the two functions are
        called <i>cont-ok</i> and <i>cont-rightmost</i>:

      <pre>
        <code>
          (s tm cont-ok cont-rightmost)
        </code>
      </pre>

      <p>
        The function name, #'s, stands for 'step'.  The tape machine, <i>tm</i>, is a
        combination of iterator and container, and #'s causes the internal iterator to
        step.  This is analogous to the container being a Turing Machine tape, the internal
        iterator a tape head, and the program, which is external to our container,
        providing the control.  The concept of state is more complicated.  To the extent
        that the external program does not keep state, state is encompassed by the
        location of the internal iterator in the container, and the values held in the
        container.

      <p>
        Note that <i>cont-ok</i> Stands for "continue OK", while <i>cont-rightmost</i>
        stands for "continue from rightmost".  At any point in time our container will
        typically have a bounded size.  When this bound is struck, we call <i>cont-rightmost</i>
        so that the program has an opportunity to overcome this bound, and implement
        Turing Complete behavior.

        <pre>
          <code>
            (s tm
              (λ()(print "everything went well"))
              (λ()(print "go get more data!"))
            )
          </code>
        </pre>

      <p>
        Functions with continuations are guaranteed never to exit directly, but instead to
        follow at least one of the continuations.  Currently there are no multi-threaded
        exits, so each library function will follow exactly one continuation.  This may
        change in the future.  A continued function typically will not have only one
        continuation, as that would be identical to calling the base function and the
        continuation function in succession.

      <p>
        For convenience we provide some general purpose continuation functions.  One of
        these is the function #'be, which returns a function that returns a given
        value. Hence,

      <pre>
        <code>
          (s tm (be t) (be ∅))
        </code>
      </pre>

      <p>
        will return true should the step succeed, and return ∅ should it not.  However
        note, that when used in this manner, step will almost surely be followed by a test
        of the return value, and that this test will almost surely be doing the same work
        that was done inside the function in order to to detect the end condition.  So one
        of the things we hope to accomplish with this library, is to eliminate this sort
        of redundant testing.

      <p>
        Tape machines can also be implemented over functions rather than containers.  So
        for example, we could have a tape machine that represents the natural numbers.
        Each time it is stepped it just goes to the next integer.  In which case there
        would be no upper bound, so we might write:

      <pre>
        <code>
          (s natural-number-generator 
            #'do-nothing
            (λ()(error 'this-cant-happen))
          )
        </code>
      </pre>

      <p>
        Here #'do-nothing is a void function.  If we ever hit a bound, then there is a bug
        in the <i>natural-number-generator</i>, so we signal an error.  Note that
        continuations are functions, and this continuation takes no arguments, so the
        error call must be wrapped in a lambda.  (It is a common mistake for beginning
        users of the library to put as a continuation argument a function that is
        evaluated to a value due to being an argument, rather than providing an argument
        that evaluates to a function.)

      <p>
        Tape machine library functions make all end cases explicitly, typically with a
        continuation argument.  We suspect this will simplify analysis tools and make life
        easier for programmers.  It is one of our objectives that code written using tape
        machines will be more amenable to formal analysis.  We discuss this further in
        another chapter.  We believe we have achieved this quality without sacrificing
        performance.


      <p>
        Like #'s above, many of the tape machine functions are single letters.  This
        facilitates stringing them together to create compound access functions, in
        analogy to lisp's car and cdr access language (cadr, caddr, etc.).  See my paper
        "Towards a Better Understanding of CAR, CDR, CADR and the Others".  The access
        language is then extended into a general out of band signaling pattern matching
        language that works on arbitrarily long data types. See the chapter on that
        subject.

      <p>
        In 2003 I designed a configurable high performance processor for Quickturn
        Technologies (one of the multiple processor designs being done by the company).
        The processor had a unique two dimensional microcode.  Instead of having a fixed
        pipeline that ran instructions, the first dimension of the microcode configured
        the available ALUs into a pipe appropriate for the given task, be it a correlation
        filter, matrix processing, etc.  The second dimension of the microcode implemented
        the instruction set over the configured ALUs.

      <p>
        I wrote an assembler for the processor, but what was missing was a high level
        programming paradigm.  The Tape machine combined with a special kind of function
        called a <i>worker</i> provides this paradigm in this library.  A worker function
        is defined to accept one or more tape machines, and to produce values for one or
        more tape machines.  Then each call of the worker takes few or no arguments,
        rather it pulls data as needed from the configured source tape machines. Per call
        it does one unit of work. It then writes the result to the configured in result
        tape machines. In the absence of dedicated hardware, a worker appears much like a
        function that has curried away its constant tape machine arguments, and is then
        called much like a step function. See the chapter on workers for more information.
        
    </section>
  </article>

</body>
</html>
